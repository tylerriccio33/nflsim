[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nflsim",
    "section": "",
    "text": "As opposed to traditional game outcome models, this is a method of resampling of similar situations, per play. By leveraging this scheme, the model pulls from strong historical precedent to form more information rich predictions.\n\n\n\n\nCapture of Subtle Effects\n\nTraditional spread models suffer from a slew of issues that is inherent in NFL games. Namely outcome variance and lack of samples.\nNFL outcomes (scores) are highly variant and subject to change. This makes modeling highly subtle interaction effects nearly impossible as most models are just struggling to estimate the main effects. This is exhibited through 2 main mechanisms.\n\nWhen regularizing tree based models, the subtle effects will be avoided in the model formula due to the inability to estimate their impact on an already complicated and variant outcome.\nLinear models will have trouble modeling these subtle effects since they usually rely on deep non-linear interactions. Regularizing models like elastic net regression in practice, will completely remove any consideration of subtle effects.\n\n\nRealism\n\nThis resampling method introduces complete realism, since the plays literally happened. By relying on historical precedent, you can expect a degree of realism that a traditional machine learning model can’t mimic.\nThis realism will beat a multi-modal or multi-output model like a neural network, since the output being modeled isn’t predicted, it’s re-sampled from the set.\n\nLeverage of Additional Information\n\nAs mentioned, traditional models are often overwhelmed by the magnitude of NFL features. However, they often fail to take advantage of this due to the outcome variance and lack of samples. This makes teasing out very deep interactions, only accessible from fringe features, difficult. By taking into account far more information w/o any performance penalty, this method can model these deep interaction effects.\n\n\n\n\n\n\nRecency Bias\n\nThe resampling technique prioritizes recent events rather than skill, unlike other models. This has far more advantages than disadvantages. However, it will fail to capture larger team changes outside of the QB.\nFor example, if there was a running back hurt all last year and they make their debut week 1, the model won’t factor this into the resampling. There is some information indicating new talent but nothing great enough (yet) to influence the decision making in a major way.\nIn the future, this problem will be slowly be alleviated.\n\nSignificant Roster Changes\n\nLike the last point, only some new information is available about significant roster changes. The model/resampling leverages existing recent information about how each team performed in some context. This context assumes at least some change in roster/personnel but it has no great way of tracking how great the change is.\nIn the future this problem will be fixed.\n\nSmall Simulation Issues\n\nThere’s no real half-time.\nHome teams are not afforded the usual 1.5 point advantage on the spread.\nHome teams always get first ball.\nKicker and punter samples tend to be very over sampled for recent events. This makes a hot hand at each position probably more influential than it needs to be.\nEach team doesn’t completely know about the clock, although it knows when it should pass and when it is losing.\nAll kickoffs are touch backs.\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#nfl-game-simulation-results",
    "href": "index.html#nfl-game-simulation-results",
    "title": "nflsim",
    "section": "",
    "text": "As opposed to traditional game outcome models, this is a method of resampling of similar situations, per play. By leveraging this scheme, the model pulls from strong historical precedent to form more information rich predictions.\n\n\n\n\nCapture of Subtle Effects\n\nTraditional spread models suffer from a slew of issues that is inherent in NFL games. Namely outcome variance and lack of samples.\nNFL outcomes (scores) are highly variant and subject to change. This makes modeling highly subtle interaction effects nearly impossible as most models are just struggling to estimate the main effects. This is exhibited through 2 main mechanisms.\n\nWhen regularizing tree based models, the subtle effects will be avoided in the model formula due to the inability to estimate their impact on an already complicated and variant outcome.\nLinear models will have trouble modeling these subtle effects since they usually rely on deep non-linear interactions. Regularizing models like elastic net regression in practice, will completely remove any consideration of subtle effects.\n\n\nRealism\n\nThis resampling method introduces complete realism, since the plays literally happened. By relying on historical precedent, you can expect a degree of realism that a traditional machine learning model can’t mimic.\nThis realism will beat a multi-modal or multi-output model like a neural network, since the output being modeled isn’t predicted, it’s re-sampled from the set.\n\nLeverage of Additional Information\n\nAs mentioned, traditional models are often overwhelmed by the magnitude of NFL features. However, they often fail to take advantage of this due to the outcome variance and lack of samples. This makes teasing out very deep interactions, only accessible from fringe features, difficult. By taking into account far more information w/o any performance penalty, this method can model these deep interaction effects.\n\n\n\n\n\n\nRecency Bias\n\nThe resampling technique prioritizes recent events rather than skill, unlike other models. This has far more advantages than disadvantages. However, it will fail to capture larger team changes outside of the QB.\nFor example, if there was a running back hurt all last year and they make their debut week 1, the model won’t factor this into the resampling. There is some information indicating new talent but nothing great enough (yet) to influence the decision making in a major way.\nIn the future, this problem will be slowly be alleviated.\n\nSignificant Roster Changes\n\nLike the last point, only some new information is available about significant roster changes. The model/resampling leverages existing recent information about how each team performed in some context. This context assumes at least some change in roster/personnel but it has no great way of tracking how great the change is.\nIn the future this problem will be fixed.\n\nSmall Simulation Issues\n\nThere’s no real half-time.\nHome teams are not afforded the usual 1.5 point advantage on the spread.\nHome teams always get first ball.\nKicker and punter samples tend to be very over sampled for recent events. This makes a hot hand at each position probably more influential than it needs to be.\nEach team doesn’t completely know about the clock, although it knows when it should pass and when it is losing.\nAll kickoffs are touch backs.\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice"
  },
  {
    "objectID": "index.html#team-level-recaps",
    "href": "index.html#team-level-recaps",
    "title": "nflsim",
    "section": "Team Level Recaps",
    "text": "Team Level Recaps\n\n\nWarning: Since gt v0.9.0, the `colors` argument has been deprecated.\n• Please use the `fn` argument instead.\nThis warning is displayed once every 8 hours.\n\n\n\n\n\n\n  \n    \n      Summary of Game Outcomes\n    \n    \n      Aggregation of high level stats across simulations per game.\n    \n    \n      \n        Teams\n      \n      \n        Summary Stats\n      \n      \n        Advanced Stats\n      \n    \n    \n      Home\n      Away\n      Home Score\n      Home Spread\n      Confidence\n      Dense Score\n      Raw Sd\n      Score Dist\n    \n  \n  \n    \n\n2.10\n−2.10\n37.44%\n13.71\n15.68\n          \n    \n\n3.22\n−3.22\n93.34%\n2.00\n7.40\n          \n    \n\n11.30\n−11.30\n0.00%\n2.24\n21.23\n          \n    \n\n0.50\n−0.50\n70.20%\n−6.72\n10.82\n          \n    \n\n−0.56\n0.56\n86.78%\n4.59\n8.37\n          \n    \n\n3.10\n−3.10\n56.05%\n4.91\n12.92\n          \n    \n\n0.56\n−0.56\n34.07%\n−7.51\n16.18\n          \n    \n\n2.70\n−2.70\n43.46%\n−7.57\n14.79\n          \n    \n\n7.10\n−7.10\n10.47%\n−2.71\n19.68\n          \n    \n\n0.80\n−0.80\n100.00%\n3.65\n6.41\n          \n    \n\n−2.89\n2.89\n42.74%\n−0.18\n14.90\n          \n    \n\n−2.10\n2.10\n70.33%\n−9.25\n10.81\n          \n    \n\n−0.89\n0.89\n89.32%\n−4.72\n7.99\n          \n    \n\n0.00\n0.00\n79.23%\n2.42\n9.49\n          \n  \n  \n  \n\n\n\n\n\n\n`summarise()` has grouped output by 'id_game'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n  \n    \n      Average QB Performance Across Simulations\n    \n    \n      Mean statistics extracted from all game replications, ranked by cpoe.\n    \n    \n      QB\n      \n      \n        Simple Metrics\n      \n      \n        Advanced Metrics\n      \n    \n    \n      dropbacks\n      passing_yards\n      tds\n      sacks\n      interceptions\n      cpoe\n      epa\n    \n  \n  \n    K.Cousins\n\n34.4\n266.3\n1.3\n1.3\n0.7\n1.32\n0.29\n    P.Mahomes\n\n43.8\n285.9\n1.7\n1.4\n0.3\n2.42\n0.27\n    J.Allen\n\n43.9\n318.7\n2.3\n2.4\n0.9\n5.92\n0.23\n    J.Garoppolo\n\n33.1\n225.5\n1.4\n2.5\n0.1\n5.98\n0.22\n    T.Lawrence\n\n35.4\n226.0\n1.8\n1.0\n0.8\n2.27\n0.17\n    J.Love\n\n29.6\n217.4\n1.2\n1.7\n1.0\n1.69\n0.16\n    J.Hurts\n\n33.0\n227.1\n1.6\n2.3\n0.4\n3.99\n0.16\n    T.Tagovailoa\n\n33.0\n218.3\n1.8\n2.2\n0.2\n2.67\n0.13\n    J.Dobbs\n\n36.4\n252.0\n1.5\n1.5\n0.9\n1.40\n0.13\n    B.Purdy\n\n30.8\n219.8\n0.9\n2.2\n0.3\n1.45\n0.13\n    D.Carr\n\n26.0\n169.8\n1.4\n1.4\n0.8\n2.39\n0.13\n    J.Fields\n\n25.8\n178.1\n1.0\n2.8\n0.9\n6.24\n0.12\n    D.Prescott\n\n34.6\n226.6\n1.3\n2.1\n0.7\n1.81\n0.12\n    J.Goff\n\n30.1\n221.0\n1.4\n1.8\n0.7\n3.03\n0.11\n    A.Richardson\n\n30.4\n217.2\n0.8\n2.1\n0.2\n−5.85\n0.10\n    M.Stafford\n\n34.1\n252.9\n1.2\n2.4\n0.6\n4.22\n0.09\n    J.Burrow\n\n38.1\n265.7\n2.1\n3.2\n0.9\n3.36\n0.08\n    S.Howell\n\n34.1\n218.6\n0.9\n3.3\n0.2\n4.75\n0.06\n    D.Jones\n\n33.7\n220.4\n0.9\n2.8\n0.8\n−0.85\n0.05\n    M.Jones\n\n28.6\n197.2\n1.1\n2.1\n0.7\n0.96\n0.03\n    L.Jackson\n\n30.8\n189.7\n1.0\n2.3\n0.5\n−2.14\n0.01\n    R.Wilson\n\n32.3\n217.9\n0.9\n2.2\n0.5\n3.19\n0.00\n    R.Tannehill\n\n22.8\n122.2\n1.0\n2.0\n0.8\n−0.10\n−0.04\n    K.Pickett\n\n30.5\n179.4\n0.6\n2.3\n0.3\n−1.36\n−0.05\n    Z.Wilson\n\n28.7\n191.0\n0.5\n1.7\n0.8\n−2.26\n−0.09\n    B.Young\n\n28.0\n133.9\n0.4\n2.4\n0.3\n−5.67\n−0.11\n    D.Ridder\n\n30.6\n146.1\n0.8\n1.6\n0.2\n−6.56\n−0.20\n    C.Stroud\n\n36.2\n218.8\n1.0\n2.8\n1.6\n−2.40\n−0.20"
  }
]