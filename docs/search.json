[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nflsim",
    "section": "",
    "text": "As opposed to traditional game outcome models, this is a method of resampling of similar situations, per play. By leveraging this scheme, the model pulls from strong historical precedent to form more information rich predictions.\n\n\n\n\nCapture of Subtle Effects\n\nTraditional spread models suffer from a slew of issues that is inherent in NFL games. Namely outcome variance and lack of samples.\nNFL outcomes (scores) are highly variant and subject to change. This makes modeling highly subtle interaction effects nearly impossible as most models are just struggling to estimate the main effects. This is exhibited through 2 main mechanisms.\n\nWhen regularizing tree based models, the subtle effects will be avoided in the model formula due to the inability to estimate their impact on an already complicated and variant outcome.\nLinear models will have trouble modeling these subtle effects since they usually rely on deep non-linear interactions. Regularizing models like elastic net regression in practice, will completely remove any consideration of subtle effects.\n\n\nRealism\n\nThis resampling method introduces complete realism, since the plays literally happened. By relying on historical precedent, you can expect a degree of realism that a traditional machine learning model can’t mimic.\nThis realism will beat a multi-modal or multi-output model like a neural network, since the output being modeled isn’t predicted, it’s re-sampled from the set.\n\nLeverage of Additional Information\n\nAs mentioned, traditional models are often overwhelmed by the magnitude of NFL features. However, they often fail to take advantage of this due to the outcome variance and lack of samples. This makes teasing out very deep interactions, only accessible from fringe features, difficult. By taking into account far more information w/o any performance penalty, this method can model these deep interaction effects.\n\n\n\n\n\n\nRecency Bias\n\nThe resampling technique prioritizes recent events rather than skill, unlike other models. This has far more advantages than disadvantages. However, it will fail to capture larger team changes outside of the QB.\nFor example, if there was a running back hurt all last year and they make their debut week 1, the model won’t factor this into the resampling. There is some information indicating new talent but nothing great enough (yet) to influence the decision making in a major way.\nIn the future, this problem will be slowly be alleviated.\n\nSignificant Roster Changes\n\nLike the last point, only some new information is available about significant roster changes. The model/resampling leverages existing recent information about how each team performed in some context. This context assumes at least some change in roster/personnel but it has no great way of tracking how great the change is.\nIn the future this problem will be fixed.\n\nSmall Simulation Issues\n\nThere’s no real half-time.\nHome teams are not afforded the usual 1.5 point advantage on the spread but are given an average of one extra possession.\nKicker and punter samples tend to be very over sampled for recent events. This makes a hot hand at each position probably more influential than it needs to be.\nEach team doesn’t completely know about the clock, although it knows when it should pass and when it is losing.\nAll kickoffs are touch backs.\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n\nWarning: package 'gtExtras' was built under R version 4.3.1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#nfl-game-simulation-results",
    "href": "index.html#nfl-game-simulation-results",
    "title": "nflsim",
    "section": "",
    "text": "As opposed to traditional game outcome models, this is a method of resampling of similar situations, per play. By leveraging this scheme, the model pulls from strong historical precedent to form more information rich predictions.\n\n\n\n\nCapture of Subtle Effects\n\nTraditional spread models suffer from a slew of issues that is inherent in NFL games. Namely outcome variance and lack of samples.\nNFL outcomes (scores) are highly variant and subject to change. This makes modeling highly subtle interaction effects nearly impossible as most models are just struggling to estimate the main effects. This is exhibited through 2 main mechanisms.\n\nWhen regularizing tree based models, the subtle effects will be avoided in the model formula due to the inability to estimate their impact on an already complicated and variant outcome.\nLinear models will have trouble modeling these subtle effects since they usually rely on deep non-linear interactions. Regularizing models like elastic net regression in practice, will completely remove any consideration of subtle effects.\n\n\nRealism\n\nThis resampling method introduces complete realism, since the plays literally happened. By relying on historical precedent, you can expect a degree of realism that a traditional machine learning model can’t mimic.\nThis realism will beat a multi-modal or multi-output model like a neural network, since the output being modeled isn’t predicted, it’s re-sampled from the set.\n\nLeverage of Additional Information\n\nAs mentioned, traditional models are often overwhelmed by the magnitude of NFL features. However, they often fail to take advantage of this due to the outcome variance and lack of samples. This makes teasing out very deep interactions, only accessible from fringe features, difficult. By taking into account far more information w/o any performance penalty, this method can model these deep interaction effects.\n\n\n\n\n\n\nRecency Bias\n\nThe resampling technique prioritizes recent events rather than skill, unlike other models. This has far more advantages than disadvantages. However, it will fail to capture larger team changes outside of the QB.\nFor example, if there was a running back hurt all last year and they make their debut week 1, the model won’t factor this into the resampling. There is some information indicating new talent but nothing great enough (yet) to influence the decision making in a major way.\nIn the future, this problem will be slowly be alleviated.\n\nSignificant Roster Changes\n\nLike the last point, only some new information is available about significant roster changes. The model/resampling leverages existing recent information about how each team performed in some context. This context assumes at least some change in roster/personnel but it has no great way of tracking how great the change is.\nIn the future this problem will be fixed.\n\nSmall Simulation Issues\n\nThere’s no real half-time.\nHome teams are not afforded the usual 1.5 point advantage on the spread but are given an average of one extra possession.\nKicker and punter samples tend to be very over sampled for recent events. This makes a hot hand at each position probably more influential than it needs to be.\nEach team doesn’t completely know about the clock, although it knows when it should pass and when it is losing.\nAll kickoffs are touch backs.\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, flatten, flatten_chr, flatten_dbl, flatten_int, flatten_lgl,\n    flatten_raw, invoke, splice\n\n\nWarning: package 'gtExtras' was built under R version 4.3.1"
  },
  {
    "objectID": "index.html#team-level-recaps",
    "href": "index.html#team-level-recaps",
    "title": "nflsim",
    "section": "Team Level Recaps",
    "text": "Team Level Recaps\n\n\n\n\n\n\n  \n    \n      Results of 150 Simulations per Game\n    \n    \n    \n      \n        Teams\n      \n      \n        Summary\n      \n      \n        Odds\n      \n      \n        Advanced Stats\n      \n    \n    \n      Home\n      Away\n      Home Score1\n      Home Points\n      Away Points\n      Confidence2\n      Home Spread\n      Total\n      Dense Score3\n      Raw Sd\n      Score Dist\n    \n  \n  \n    \n\n4.4\n23.1\n19.0\n33.69%\n−4.4\n42.1\n5.2\n12.4\n          \n    \n\n4.0\n18.0\n13.8\n66.43%\n−4.0\n31.8\n1.9\n10.6\n          \n    \n\n6.9\n28.9\n22.9\n50.49%\n−6.9\n51.8\n5.3\n11.5\n          \n    \n\n4.3\n20.4\n16.3\n74.99%\n−4.3\n36.7\n6.0\n10.1\n          \n    \n\n0.7\n19.0\n18.3\n67.55%\n−0.7\n37.4\n0.5\n10.5\n          \n    \n\n4.9\n22.4\n16.6\n35.73%\n−4.9\n39.0\n0.3\n12.3\n          \n    \n\n−1.3\n22.3\n23.5\n21.55%\n1.3\n45.8\n4.3\n13.1\n          \n    \n\n1.4\n21.2\n19.9\n43.47%\n−1.4\n41.0\n−1.5\n11.9\n          \n    \n\n2.7\n22.0\n18.5\n49.05%\n−2.7\n40.5\n−0.4\n11.6\n          \n    \n\n−0.3\n24.8\n25.4\n83.33%\n0.3\n50.1\n1.5\n9.7\n          \n    \n\n−0.7\n17.3\n18.3\n100.00%\n0.7\n35.6\n−0.5\n8.7\n          \n    \n\n−6.7\n13.9\n20.6\n9.29%\n6.7\n34.5\n−7.8\n13.7\n          \n    \n\n−2.9\n21.6\n24.1\n0.00%\n2.9\n45.7\n−3.5\n14.3\n          \n  \n  \n    \n               = Winner\n    \n  \n  \n    \n      \n        1 Interpreted as a home win by this much. 2 Function of relative variance in the results, also the Raw SD. Higher numbers indicate higher ranges of outcomes, or uncertainty. 3 Adjusted results reflecting the most dense location in the score distribution. Numbers should closely reflect the average, if distributed normally.\n      \n    \n  \n\n\n\n\n\n\n`summarise()` has grouped output by 'id_game'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n  \n    \n      Average QB Performance Across Simulations\n    \n    \n      Mean statistics extracted from all game replications, ranked by cpoe.\n    \n    \n      QB\n      \n      \n        Simple Metrics\n      \n      \n        Advanced Metrics\n      \n    \n    \n      dropbacks\n      passing_yards\n      tds\n      sacks\n      interceptions\n      cpoe\n      epa\n    \n  \n  \n    P.Mahomes\n\n44.3\n305.7\n2.3\n1.5\n0.5\n4.36\n0.26\n    J.Garoppolo\n\n31.9\n237.1\n1.5\n1.4\n0.8\n1.43\n0.22\n    J.Allen\n\n44.0\n302.6\n1.7\n2.4\n0.8\n4.05\n0.19\n    T.Tagovailoa\n\n31.7\n231.7\n1.7\n1.7\n0.4\n2.74\n0.19\n    K.Cousins\n\n39.4\n265.1\n1.8\n2.2\n0.7\n1.89\n0.16\n    J.Love\n\n31.3\n224.0\n1.7\n2.2\n0.7\n0.71\n0.16\n    A.Richardson\n\n32.4\n233.0\n1.2\n1.7\n0.6\n−0.98\n0.15\n    D.Prescott\n\n35.8\n234.1\n1.5\n2.3\n0.6\n2.29\n0.13\n    M.Stafford\n\n32.8\n229.9\n1.4\n2.1\n0.6\n3.14\n0.13\n    B.Purdy\n\n31.8\n210.4\n1.2\n2.3\n0.5\n3.00\n0.12\n    T.Lawrence\n\n39.8\n262.0\n1.8\n2.4\n0.6\n2.34\n0.12\n    J.Dobbs\n\n34.1\n239.4\n1.3\n1.4\n0.9\n1.88\n0.11\n    D.Jones\n\n32.9\n204.4\n1.0\n2.3\n0.5\n1.85\n0.11\n    J.Goff\n\n30.3\n208.8\n1.0\n1.6\n0.5\n1.50\n0.09\n    R.Wilson\n\n31.2\n210.3\n1.3\n2.2\n0.3\n1.43\n0.09\n    J.Hurts\n\n32.2\n213.0\n1.2\n1.8\n0.5\n1.12\n0.09\n    L.Jackson\n\n27.1\n162.3\n1.0\n1.7\n0.3\n0.44\n0.06\n    D.Carr\n\n29.1\n182.1\n0.9\n1.9\n0.4\n−0.03\n0.06\n    J.Burrow\n\n36.1\n221.0\n1.5\n2.1\n0.7\n0.91\n0.06\n    M.Jones\n\n29.1\n196.8\n0.8\n2.1\n0.6\n0.49\n0.01\n    Z.Wilson\n\n35.1\n211.6\n0.7\n2.5\n0.9\n−4.96\n−0.05\n    K.Pickett\n\n33.0\n186.5\n0.5\n2.8\n0.7\n0.06\n−0.06\n    R.Tannehill\n\n27.2\n155.8\n0.8\n2.3\n0.8\n0.30\n−0.08\n    C.Stroud\n\n33.4\n180.8\n0.9\n2.5\n0.9\n−0.63\n−0.09\n    B.Young\n\n27.0\n144.4\n0.4\n1.9\n0.4\n−0.30\n−0.09\n    D.Ridder\n\n27.0\n152.8\n0.6\n2.3\n0.2\n−1.51\n−0.11\n  \n  \n  \n\n\n\n\n\nDeep Dives\n\n\n`summarise()` has grouped output by 'id_game', 'winning_team'. You can override\nusing the `.groups` argument.\n\n\n\n\n\n\n  \n    \n      Keys to the Game\n    \n    \n      Trends and formulas driving wins and losses. Some teams will not be represented, directly.\n    \n    \n      \n      Value/Rank\n      \n    \n  \n  \n    \n      2023_05_BAL_PIT\n    \n    \n0.3\n2/32\nMinimize the interceptions - wins included a low int rate\n    \n3.3\n1/32\nProtect the QB - wins were associated with low qb hit rates.\n    \n0.1\n3/32\nBe run efficient - wins were driven by an consistently effective run game.\n    \n      2023_05_CAR_DET\n    \n    \n4.2\n5/32\nProtect the QB - wins were associated with low qb hit rates.\n    \n0.1\n2/32\nBe run efficient - wins were driven by an consistently effective run game.\n    \n      2023_05_CIN_ARI\n    \n    \n1,929.7\n2/32\nControl the game - time of possession was important to winning games.\n    \n2.9\n3/32\nThrow it early - wins were associated with higher early down passing over expected.\n    \n0.5\n5/32\nBe an effective passer - wins were lead by highly effective passing games.\n    \n      2023_05_DAL_SF\n    \n    \n8.4\n5/32\nAir it out - wins included high average air yards.\n    \n0.5\n4/32\nBe an effective passer - wins were lead by highly effective passing games.\n    \n0.1\n4/32\nBe run efficient - wins were driven by an consistently effective run game.\n    \n0.1\n1/32\nBe run efficient - wins were driven by an consistently effective run game.\n    \n      2023_05_GB_LV\n    \n    \n1,848.5\n4/32\nControl the game - time of possession was important to winning games.\n    \n0.0\n2/32\nLean on YAC - win on the back of consistent yac.\n    \n0.1\n5/32\nBe run efficient - wins were driven by an consistently effective run game.\n    \n      2023_05_HOU_ATL\n    \n    \n9.0\n1/32\nAir it out - wins included high average air yards.\n    \n0.2\n1/32\nMinimize the interceptions - wins included a low int rate\n    \n3.8\n2/32\nProtect the QB - wins were associated with low qb hit rates.\n    \n1,899.4\n3/32\nControl the game - time of possession was important to winning games.\n    \n      2023_05_JAX_BUF\n    \n    \n8.5\n3/32\nAir it out - wins included high average air yards.\n    \n9.6\n1/32\nThrow it early - wins were associated with higher early down passing over expected.\n    \n0.6\n2/32\nBe an effective passer - wins were lead by highly effective passing games.\n    \n3.3\n1/32\nHit paydirt - games were won by touchdowns, not field goals.\n    \n1,990.1\n1/32\nControl the game - time of possession was important to winning games.\n    \n1.7\n4/32\nThrow it early - wins were associated with higher early down passing over expected.\n    \n      2023_05_KC_MIN\n    \n    \n9.2\n2/32\nThrow it early - wins were associated with higher early down passing over expected.\n    \n0.6\n1/32\nBe an effective passer - wins were lead by highly effective passing games.\n    \n2.9\n3/32\nHit paydirt - games were won by touchdowns, not field goals.\n    \n0.0\n5/32\nThrow it early - wins were associated with higher early down passing over expected.\n    \n0.5\n3/32\nBe an effective passer - wins were lead by highly effective passing games.\n    \n2.7\n4/32\nHit paydirt - games were won by touchdowns, not field goals.\n    \n      2023_05_NO_NE\n    \n    \n0.4\n5/32\nMinimize the interceptions - wins included a low int rate\n    \n0.0\n3/32\nLean on YAC - win on the back of consistent yac.\n    \n      2023_05_NYG_MIA\n    \n    \n0.3\n3/32\nMinimize the interceptions - wins included a low int rate\n    \n2.7\n5/32\nHit paydirt - games were won by touchdowns, not field goals.\n    \n      2023_05_NYJ_DEN\n    \n    \n8.5\n4/32\nAir it out - wins included high average air yards.\n    \n0.4\n4/32\nMinimize the interceptions - wins included a low int rate\n    \n1,845.2\n5/32\nControl the game - time of possession was important to winning games.\n    \n0.0\n4/32\nLean on YAC - win on the back of consistent yac.\n    \n0.0\n5/32\nLean on YAC - win on the back of consistent yac.\n    \n      2023_05_PHI_LA\n    \n    \n4.0\n3/32\nProtect the QB - wins were associated with low qb hit rates.\n    \n2.9\n2/32\nHit paydirt - games were won by touchdowns, not field goals.\n    \n      2023_05_TEN_IND\n    \n    \n8.6\n2/32\nAir it out - wins included high average air yards.\n    \n0.0\n1/32\nLean on YAC - win on the back of consistent yac.\n    \n4.1\n4/32\nProtect the QB - wins were associated with low qb hit rates.\n  \n  \n  \n    \n       \n    \n  \n\n\n\n\n\n\nWR Stats\n\n\n`summarise()` has grouped output by 'id_game'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nReceiving Statistics\nSummary stats across any receiver player for all simulations."
  }
]